[
    {
        "id": "2210.11610",
        "title": "Large Language Models Can Self-Improve",
        "date_published": "25 Oct 2022",
        "abstract": "Large Language Models (LLMs) have achieved excellent performances in various tasks. However, fine-tuning an LLM requires extensive supervision. Human, on the other hand, may improve their reasoning abilities by self-thinking without external inputs. In this work, we demonstrate that an LLM is also capable of self-improving with only unlabeled datasets. We use a pre-trained LLM to generate 'high-confidence' rationale-augmented answers for unlabeled questions using Chain-of-Thought prompting and self-consistency, and fine-tune the LLM using those self-generated solutions as target outputs. We show that our approach improves the general reasoning ability of a 540B-parameter LLM (74.4%->82.1% on GSM8K, 78.2%->83.0% on DROP, 90.0%->94.4% on OpenBookQA, and 63.4%->67.9% on ANLI-A3) and achieves state-of-the-art-level performance, without any ground truth label. We conduct ablation studies and show that fine-tuning on reasoning is critical for self-improvement.",
        "conclusion": "We demonstrated that a Large Language Model (LLM) is capable of improving its performance on reasoning datasets by training on its own generated labels, given input questions only. Experiments using an LLM with 540 billion parameters show that our approach improves the accuracy scores on the six datasets by 1.1% to 7.7%, achieving new state-of-the-art results on ARC, OpenBookQA, and ANLI, without training on ground truth labels. Furthermore, we show that it is possible for the LLM to self-improve even on its own generated questions and few-shot Chain-of-Thought prompts. As part of our future work, we plan to combine large-scale generated data from our approach and existing supervised data, to further improve the performance of LLMs.",
        "authors": ["Jiaxin Huang", "Shixiang Shane Gu", "Le Hou", "Yuexin Wu", "Xuezhi Wang", "Hongkun Yu", "Jiawei Han"],
        "number_of_citations": 416,
        "datasets": ["GSM8K", "DROP", "OpenBookQA", "ANLI-A3"],
        "domains": ["Computer Science", "Computation and Language"],
        "keywords": ["Large Language Models", "self-improvement", "Chain-of-Thought prompting", "self-consistency", "fine-tuning", "reasoning ability"],
        "conference": "EMNLP 2023",
        "github_repo": null,
        "url": "https://arxiv.org/pdf/2210.11610"
    },
    {
        "id": "2310.16040",
        "title": "Instruct and Extract: Instruction Tuning for On-Demand Information Extraction",
        "date_published": "24 Oct 2023",
        "abstract": "Large language models with instruction-following capabilities open the door to a wider group of users. However, when it comes to information extraction - a classic task in natural language processing - most task-specific systems cannot align well with long-tail ad hoc extraction use cases for non-expert users. To address this, we propose a novel paradigm, termed On-Demand Information Extraction, to fulfill the personalized demands of real-world users. Our task aims to follow the instructions to extract the desired content from the associated text and present it in a structured tabular format. The table headers can either be user-specified or inferred contextually by the model. To facilitate research in this emerging area, we present a benchmark named InstructIE, inclusive of both automatically generated training data, as well as the human-annotated test set. Building on InstructIE, we further develop an On-Demand Information Extractor, ODIE. Comprehensive evaluations on our benchmark reveal that ODIE substantially outperforms the existing open-source models of similar size. Our code and dataset are released on https://github.com/yzjiao/On-Demand-IE.",
        "conclusion": "As part of our future work, we plan to combine large-scale generated data from our approach and existing supervised data, to further improve the performance of LLMs.",
        "authors": ["Yizhu Jiao", "Ming Zhong", "Sha Li", "Ruining Zhao", "Siru Ouyang", "Heng Ji", "Jiawei Han"],
        "number_of_citations": 21,
        "datasets": ["InstructIE"],
        "domains": ["Computer Science", "Natural Language Processing", "Information Extraction"],
        "keywords": ["On-Demand Information Extraction", "Instruction Tuning", "Large Language Models"],
        "conference": "EMNLP 2023",
        "github_repo": "https://github.com/yzjiao/On-Demand-IE",
        "url": "https://arxiv.org/pdf/2310.16040"
    },
    {
        "id": "2402.01030",
        "title": "Executable Code Actions Elicit Better LLM Agents",
        "date_published": "1 Feb 2024",
        "abstract": "Large Language Model (LLM) agents, capable of performing a broad range of actions, such as invoking tools and controlling robots, show great potential in tackling real-world challenges. LLM agents are typically prompted to produce actions by generating JSON or text in a pre-defined format, which is usually limited by constrained action space (e.g., the scope of pre-defined tools) and restricted flexibility (e.g., inability to compose multiple tools). This work proposes to use executable Python code to consolidate LLM agents' actions into a unified action space (CodeAct). Integrated with a Python interpreter, CodeAct can execute code actions and dynamically revise prior actions or emit new actions upon new observations through multi-turn interactions. Our extensive analysis of 17 LLMs on APIBank and a newly curated benchmark shows that CodeAct outperforms widely used alternatives (up to 20% higher success rate). The encouraging performance of CodeAct motivates us to build an open-source LLM agent that interacts with environments by executing interpretable code and collaborates with users using natural language. To this end, we collect an instruction-tuning dataset CodeActInstruct that consists of 7k multi-turn interactions using CodeAct. We show that it can be used with existing data to improve models in agent-oriented tasks without compromising their general capability. CodeActAgent, finetuned from Llama2 and Mistral, is integrated with Python interpreter and uniquely tailored to perform sophisticated tasks (e.g., model training) using existing libraries and autonomously self-debug.",
        "conclusion": "As part of our future work, we plan to combine large-scale generated data from our approach and existing supervised data, to further improve the performance of LLMs.",
        "authors": ["Xingyao Wang", "Yangyi Chen", "Lifan Yuan", "Yizhe Zhang", "Yunzhu Li", "Hao Peng", "Heng Ji"],
        "number_of_citations": 51,
        "datasets": ["APIBank", "CodeActInstruct"],
        "domains": ["Computer Science", "Natural Language Processing", "Machine Learning", "Artificial Intelligence"],
        "keywords": ["Large Language Models", "Code Generation", "Agents", "Instruction Tuning"],
        "conference": "ICML 2024",
        "github_repo": "https://github.com/xingyaoww/code-act",
        "url": "https://arxiv.org/pdf/2402.01030"
    },
    {
        "id": "2310.00898",
        "title": "Enabling Language Models to Implicitly Learn Self-Improvement",
        "date_published": "2024",
        "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in open-ended text generation tasks. However, the inherent open-ended nature of these tasks implies that there is always room for improvement in the quality of model responses. To address this challenge, various approaches have been proposed to enhance the performance of LLMs. There has been a growing focus on enabling LLMs to self-improve their response quality, thereby reducing the reliance on extensive human annotation efforts for collecting diverse and high-quality training data. Recently, prompting-based methods have been widely explored among self-improvement methods owing to their effectiveness, efficiency, and convenience. However, those methods usually require explicitly and thoroughly written rubrics as inputs to LLMs. It is expensive and challenging to manually derive and provide all necessary rubrics with a real-world complex goal for improvement (e.g., being more helpful and less harmful). To this end, we propose an ImPlicit Self-ImprovemenT (PIT) framework that implicitly learns the improvement goal from human preference data. PIT only requires preference data that are used to train reward models without extra human efforts. Specifically, we reformulate the training objective of reinforcement learning from human feedback (RLHF) -- instead of maximizing response quality for a given input, we maximize the quality gap of the response conditioned on a reference response. In this way, PIT is implicitly trained with the improvement goal of better aligning with human preferences. Experiments on two real-world datasets and one synthetic dataset show that our method significantly outperforms prompting-based methods.",
        "conclusion": null,
        "authors": ["Ziqi Wang", "Le Hou", "Tianjian Lu", "Yuexin Wu", "Yunxuan Li", "Hongkun Yu", "Heng Ji"],
        "number_of_citations": 2,
        "datasets": ["Anthropic/HH-RLHF", "OpenAI/Summary", "synthetic data"],
        "domains": ["Computer Science", "Natural Language Processing", "Machine Learning"],
        "keywords": ["Large Language Models", "Reinforcement Learning", "Human Feedback", "Self-Improvement"],
        "conference": "ICLR 2024",
        "github_repo": null,
        "url": "https://arxiv.org/pdf/2310.00898"
    },
    {
        "id": "2402.13364",
        "title": "A Simple but Effective Approach to Improve Structured Language Model Output for Information Extraction",
        "date_published": "20 Feb 2024",
        "abstract": "Large language models (LLMs) have demonstrated impressive abilities in generating unstructured natural language according to instructions. However, their performance can be inconsistent when tasked with producing text that adheres to specific structured formats, which is crucial in applications like named entity recognition (NER) or relation extraction (RE). To address this issue, this paper introduces an efficient method, G&O, to enhance their structured text generation capabilities. It breaks the generation into a two-step pipeline: initially, LLMs generate answers in natural language as intermediate responses. Subsequently, LLMs are asked to organize the output into the desired structure, using the intermediate responses as context. G&O effectively separates the generation of content from the structuring process, reducing the pressure of completing two orthogonal tasks simultaneously. Tested on zero-shot NER and RE, the results indicate a significant improvement in LLM performance with minimal additional efforts.",
        "conclusion": "The G&O method effectively improves the performance of language models in generating structured outputs, demonstrating its utility for various structured text generation tasks.",
        "authors": ["Yinghao Li", "Rampi Ramprasad", "Chao Zhang"],
        "number_of_citations": 5,
        "datasets": ["CoNLL 2003", "NCBI Disease", "BC5CDR", "PolyIE", "CoNLL 2004", "NYT"],
        "domains": ["Computer Science", "Natural Language Processing", "Information Extraction"],
        "keywords": ["Large Language Models", "Information Extraction", "Named Entity Recognition", "Relation Extraction", "Structured Text Generation"],
        "conference": "Preprint",
        "github_repo": null,
        "url": "https://arxiv.org/pdf/2402.13364"
    },
    {
        "id": "2307.01448",
        "title": "ReactIE: Enhancing Chemical Reaction Extraction with Weak Supervision",
        "date_published": "4 Jul 2023",
        "abstract": "Structured chemical reaction information plays a vital role for chemists engaged in laboratory work and advanced endeavors such as computer-aided drug design. Despite the importance of extracting structured reactions from scientific literature, data annotation for this purpose is cost-prohibitive due to the significant labor required from domain experts. Consequently, the scarcity of sufficient training data poses an obstacle to the progress of related models in this domain. In this paper, we propose ReactIE, which combines two weakly supervised approaches for pre-training. Our method utilizes frequent patterns within the text as linguistic cues to identify specific characteristics of chemical reactions. Additionally, we adopt synthetic data from patent records as distant supervision to incorporate domain knowledge into the model. Experiments demonstrate that ReactIE achieves substantial improvements and outperforms all existing baselines.",
        "conclusion": "In this paper, we present REACTIE, an automatic framework for extracting chemical reactions from the scientific literature. Our approach incorpo- rates linguistic and chemical knowledge into the pre-training. Experiments show that REACTIE achieves state-of-the-art results by a large margin.",
        "authors": ["Ming Zhong", "Siru Ouyang", "Minhao Jiang", "Vivian Hu", "Yizhu Jiao", "Xuan Wang", "Jiawei Han"],
        "number_of_citations": 7,
        "datasets": ["Reaction Corpus"],
        "domains": ["Computer Science", "Chemistry", "Natural Language Processing", "Information Extraction"],
        "keywords": ["Chemical Reaction Extraction", "Weak Supervision", "Distant Supervision", "Pre-training"],
        "conference": "ACL 2023",
        "github_repo": null,
        "url": "https://arxiv.org/pdf/2307.01448"
    },
    {
        "id": "EMNLP2023_ReactionMiner",
        "title": "Reaction Miner: An Integrated System for Chemical Reaction Extraction from Textual Data",
        "date_published": "December 2023",
        "abstract": " Chemical reactions, as a core entity in the realm of chemistry, hold crucial implications in diverse areas ranging from hands-on laboratory research to advanced computational drug design. Despite a burgeoning interest in employing NLP techniques to extract these reactions, aligning this task with the real-world requirements of chemistry practitioners remains an ongoing challenge. In this paper, we present Reaction Miner, a system specifically designed to interact with raw scientific literature, delivering precise and more informative chemical reactions. Going beyond mere extraction, Reaction Miner integrates a holistic workflow: it accepts PDF files as input, bypassing the need for pre-processing and bolstering user accessibility. Subsequently, a text segmentation module ensures that the refined text encapsulates complete chemical reactions, augmenting the accuracy of extraction. Moreover, Reaction Miner broadens the scope of existing pre-defined reaction roles, including vital attributes previously neglected, thereby offering a more comprehensive depiction of chemical reactions. Evaluations conducted by chemistry domain users highlight the efficacy of each module in our system, demonstrating Reaction Miner as a powerful tool in this field.",
        "conclusion": "In our exploration, we present REACTION MINER, an integrated system adept at extracting chemical reactions directly from raw scientific PDFs. Be- yond mere extraction, it offers enhanced accuracy by broadening the scope of reaction roles and elimi- nating prior gaps. Feedback from chemistry experts marks it as a powerful tool for the field.",
        "authors": ["Ming Zhong", "Siru Ouyang", "Yizhu Jiao", "Priyanka Kargupta", "Leo Luo", "Yanzhen Shen", "Bobby Zhou", "Xianrui Zhong", "Xuan Liu", "Hongxiang Li", "Jinfeng Xiao", "Minhao Jiang", "Vivian Hu", "Xuan Wang", "Heng Ji", "Martin Burke", "Huimin Zhao", "Jiawei Han"],
        "number_of_citations": 4,
        "datasets": [],
        "domains": ["Natural Language Processing", "Chemistry", "Information Extraction"],
        "keywords": ["Chemical reaction extraction", "PDF processing", "text segmentation", "reaction roles"],
        "conference": "EMNLP 2023",
        "github_repo": "https://github.com/maszhongming/ReactionMiner",
        "url": "https://aclanthology.org/2023.emnlp-demo.36.pdf"
    }
]